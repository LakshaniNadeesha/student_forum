{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449113c4-2879-427e-8fdf-8861ae2ff56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and other non-alphabetic characters\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e0fa29-f7cc-4690-84b6-8c3cca0873ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Named Entities\n",
    "from nltk import ne_chunk\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    named_entities = ne_chunk(tagged_tokens)\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21fe3844-199c-4941-a83d-cfe9137ec3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Named Entities\n",
    "def count_named_entities(text):\n",
    "    named_entities = extract_named_entities(text)\n",
    "    count = sum(1 for chunk in named_entities if hasattr(chunk, 'label'))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54637694-a235-420a-9467-2cdac773ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cohesion\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def compute_cohesion(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    words = [word for sentence in tokenized_sentences for word in sentence if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    # Compute word overlap between consecutive sentences\n",
    "    word_overlap_count = 0\n",
    "    for i in range(len(tokenized_sentences) - 1):\n",
    "        sentence1 = set(tokenized_sentences[i])\n",
    "        sentence2 = set(tokenized_sentences[i + 1])\n",
    "        word_overlap_count += len(sentence1.intersection(sentence2))\n",
    "\n",
    "    # Compute cohesion as the total word overlap normalized by the total number of words\n",
    "    total_words = len(words)\n",
    "    cohesion = word_overlap_count / total_words if total_words > 0 else 0\n",
    "\n",
    "    return cohesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "226c339d-25a4-422c-923f-e8c4b945c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated Coherence\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def calculate_coherence(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word.lower() for sentence in sentences for word in word_tokenize(sentence)]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    # Calculate bigrams\n",
    "    bi_grams = list(bigrams(words))\n",
    "\n",
    "    # Calculate frequency distribution of words and bigrams\n",
    "    freq_dist_words = FreqDist(words)\n",
    "    freq_dist_bigrams = FreqDist(bi_grams)\n",
    "\n",
    "    # Calculate Pointwise Mutual Information (PMI)\n",
    "    coherence = sum([freq_dist_bigrams[bigram] * freq_dist_words[bigram[0]] * freq_dist_words[bigram[1]] for bigram in bi_grams])\n",
    "\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60488fdd-ef95-4a11-8698-ef371112a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count of a given text\n",
    "\n",
    "def word_count(text):\n",
    "    # Use split() to break the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Count the number of words\n",
    "    count = len(words)\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86026e9b-d1eb-48f1-91cd-2e9026a176b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C-EX'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "def average_word2vec(tokens, model, vector_size):\n",
    "    vector_sum = sum(model.wv[word] for word in tokens if word in model.wv)\n",
    "    return vector_sum / len(tokens) if len(tokens) > 0 else np.zeros(vector_size)\n",
    "\n",
    "def preprocess_input_for_prediction(message, parent, student, created, subject, word2vec_model):\n",
    "    # Create a DataFrame with user inputs\n",
    "    data = pd.DataFrame({'message': [message],\n",
    "                         'parent': [parent],\n",
    "                         'student': [student],\n",
    "                         'created': [created],\n",
    "                         'subject': [subject]})\n",
    "\n",
    "    # Preprocess the 'created' column\n",
    "    data['created'] = pd.to_datetime(data['created'])\n",
    "    data['hour'] = data['created'].dt.hour\n",
    "\n",
    "    # Additional feature computations\n",
    "    data['named_entities_count'] = data['message'].apply(count_named_entities)\n",
    "    data['cohesion'] = data['message'].apply(compute_cohesion)\n",
    "    data['coherence'] = data['message'].apply(calculate_coherence)\n",
    "    data['wc'] = data['message'].apply(word_count)\n",
    "\n",
    "    # Combine text features\n",
    "    data['combined_text'] = data[['parent', 'student', 'subject', 'message']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    # Tokenize the text\n",
    "    data['tokenized_text'] = data['combined_text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "    # Create Word2Vec vectors for each text using the provided model\n",
    "    data['word2vec'] = data['tokenized_text'].apply(lambda x: average_word2vec(x, word2vec_model, vector_size=100))\n",
    "\n",
    "    # Convert Word2Vec vectors to DataFrame columns\n",
    "    word2vec_columns = pd.DataFrame(data['word2vec'].to_list(), columns=[f'w2v_{i}' for i in range(100)])\n",
    "\n",
    "    # Combine Word2Vec features, hour, and other features\n",
    "    X_combined = pd.concat([word2vec_columns, data[['hour', 'named_entities_count', 'cohesion', 'coherence', 'wc']]], axis=1)\n",
    "\n",
    "    return X_combined\n",
    "\n",
    "def predict_label_with_word2vec(message, parent, student, created, subject, rf_model, word2vec_model):\n",
    "    # Preprocess the input for prediction\n",
    "    input_data = preprocess_input_for_prediction(message, parent, student, created, subject, word2vec_model)\n",
    "\n",
    "    # Make predictions using the pre-trained Random Forest model\n",
    "    prediction = rf_model.predict(input_data)\n",
    "\n",
    "    return prediction[0]\n",
    "\n",
    "# Example usage:\n",
    "user_message = \"Your user input message\"\n",
    "user_parent = \"student 12\"\n",
    "user_student = \"student 23\"\n",
    "user_created = \"2023-01-01 12:30:00\"  # Format: \"YYYY-MM-DD HH:mm:ss\"\n",
    "user_subject = \"Users subject input\"\n",
    "\n",
    "# Load the Random Forest model and Word2Vec model from the saved file\n",
    "with open('rf_model.pickle', 'rb') as file:\n",
    "    rf_model, word2vec_model = pickle.load(file)\n",
    "\n",
    "predict_label_with_word2vec(user_message, user_parent, user_student, user_created, user_subject, rf_model, word2vec_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295e06c-b751-42da-a810-1cf3f20d8c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
